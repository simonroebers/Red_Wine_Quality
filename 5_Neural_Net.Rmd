---
title: '5 Neural Net Prediction'
output:
  github_document:
    toc: true
    toc_depth: 2
---

# Load packages and data
```{r}
library(tidymodels)
library(neuralnet)
library(doParallel)
df <- read.csv('winequality-red.csv')
glimpse(df)
```

# Create train split, test split and cross validation
```{r}
set.seed(1)
wine_split <- initial_split(df, prop = 0.7)
wine_split

wine_train <- training(wine_split)
wine_test  <- testing(wine_split)

cv_folds <- wine_train |> vfold_cv(v = 3)
```

- Data set is split into 70% training data and 30% test data
- Training data is divided into 3-fold cross validation

# Neural Network
## Set model and workflow
```{r}
# neural_network <- mlp(hidden_units = tune(), epochs = 7500, activation = "relu") |> 
#   set_engine("keras", verbose = 0, batch_size = 32, metrics = c("mse", "mae")) |> 
#   set_mode("regression")
# neural_network_recipe <- recipe(quality ~ alcohol + sulphates + volatile.acidity,
#   data = df) |>  step_normalize(all_predictors())
# neural_network_workflow <- workflow() |>  add_recipe(neural_network_recipe) |>  
#   add_model(neural_network)
```

- Neural net model is defined and combined into a workflow
- Variables are normalized to not bias distance calculations of the model
- The number of hidden units will be tuned with cross validation

## Tune model
```{r}
# tune_grid <- tibble(hidden_units = 3:4)
# neural_net_tune <- tune_grid(neural_network_workflow, resamples = cv_folds,
#   grid = tune_grid, metrics = metric_set(rmse, mae),
#   control = control_grid(verbose = FALSE)
# )
```

# Neural Network
## Set model and workflow
```{r}
neural_network <- neuralnet(hidden = tune()) |>  
  # mlp(hidden_units = tune()) |>  
  set_engine("nnet") |> set_mode("regression")
neural_network_recipe <- recipe(quality ~ alcohol + sulphates + volatile.acidity,
  data = df) |>  step_normalize(all_predictors())
neural_network_workflow <- workflow() |>  add_recipe(neural_network_recipe) |>  
  add_model(neural_network)
```

- Neural net model is defined and combined into a workflow
- Variables are normalized to not bias distance calculations of the model
- The number of hidden units will be tuned with cross validation

## Tune model
```{r}
tune_grid <- tibble(hidden = 1:10)
# tune_grid <- tibble(hidden_units = 1:10)
registerDoParallel()
neural_net_tune <- tune_grid(neural_network_workflow, resamples = cv_folds,
  grid = tune_grid, metrics = metric_set(rmse, mae, rsq_trad))
```

## Evaluation of tuning results
```{r}
neural_net_tune %>% collect_metrics() %>%
  ggplot(aes(x = hidden_units, y = mean, ymin = mean - std_err, ymax = mean + std_err, 
             colour = .metric)) +
  geom_errorbar() + geom_line() + geom_point() +
  facet_wrap(~ .metric, scales = "free_y")
neural_net_tune |> show_best("rmse", n = 5) 
neural_net_selected <- select_by_one_std_err(neural_net_tune, metric = "rmse", hidden_units)
neural_net_selected
neural_net_workflow_final <- finalize_workflow(neural_network_workflow, neural_net_selected)
neural_net_workflow_final
```

# Determine model performance
```{r}

stopImplicitCluster()
```

